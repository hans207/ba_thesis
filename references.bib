
@article{duff_2001,
	title = {Monte-Carlo Algorithms for the Improvement of Finite-State Controllers: Application to Bayes-Adaptive Markov Decision Processes},
	shorttitle = {Monte-Carlo Algorithms for the Improvement of Finite-State Controllers},
	abstract = {We consider the problem of "optimal learning" for Markov decision processes with uncertain  transition probabilities. Motivated  by the correspondence between these processes  and partially-observable Markov decision  processes, we adopt policies expressed  as finite-state stochastic automata, and we  propose policy improvement algorithms that  utilize Monte-Carlo techniques for gradient  estimation and ascent.},
	journaltitle = {International Workshop on Artificial Intelligence and Statistics},
	author = {Duff, M. O.},
	date = {2001}
}

@article{guez_long,
	title = {Scalable and Efficient Bayes-Adaptive Reinforcement Learning Based on Monte-Carlo Tree Search},
	volume = {48},
	issn = {1076-9757},
	url = {https://jair.org/index.php/jair/article/view/10853},
	doi = {10.1613/jair.4117},
	pages = {841--883},
	journaltitle = {Journal of Artificial Intelligence Research},
	author = {Guez, A. and Silver, D. and Dayan, P.},
	urldate = {2018-09-12},
	date = {2013}
}

@article{survey_2015,
	title = {Bayesian Reinforcement Learning: A Survey},
	volume = {8},
	url = {http://arxiv.org/abs/1609.04436},
	doi = {10.1561/2200000049},
	shorttitle = {Bayesian Reinforcement Learning},
	abstract = {Bayesian methods for machine learning have been widely investigated, yielding principled methods for incorporating prior information into inference algorithms. In this survey, we provide an in-depth review of the role of Bayesian methods for the reinforcement learning ({RL}) paradigm. The major incentives for incorporating Bayesian reasoning in {RL} are: 1) it provides an elegant approach to action-selection (exploration/exploitation) as a function of the uncertainty in learning; and 2) it provides a machinery to incorporate prior knowledge into the algorithms. We first discuss models and methods for Bayesian inference in the simple single-step Bandit model. We then review the extensive recent literature on Bayesian methods for model-based {RL}, where prior information can be expressed on the parameters of the Markov model. We also present Bayesian methods for model-free {RL}, where priors are expressed over the value function or policy class. The objective of the paper is to provide a comprehensive survey on Bayesian {RL} algorithms and their theoretical and empirical properties.},
	pages = {359--492},
	number = {5},
	journaltitle = {Foundations and Trends® in Machine Learning},
	author = {Ghavamzadeh, M. and Mannor, S. and Pineau, J. and Tamar, A.},
	date = {2015},
	eprinttype = {arxiv},
	eprint = {1609.04436}
}

@thesis{duff_2002,
	title = {Optimal Learning: Computational procedures for Bayes-adaptive Markov decision processes},
	shorttitle = {Optimal Learning},
	institution = {University of Massachusetts at Amherst},
	type = {phdthesis},
	author = {Duff, M. O. and Barto, A.},
	date = {2002}
}

@book{russell_artificial_2003,
	location = {Upper Saddle River, {NJ}},
	edition = {2. ed., internat. ed},
	title = {Artificial intelligence: a modern approach ; [the intelligent agent book]},
	isbn = {978-0-13-080302-3},
	series = {Prentice Hall series in artificial intelligence},
	shorttitle = {Artificial intelligence},
	pagetotal = {1081},
	publisher = {Prentice Hall},
	author = {Russell, S. and Norvig, P.},
	date = {2003},
	note = {{OCLC}: 249298222}
}

@book{sutton_barto,
	location = {Cambridge, {UNITED} {STATES}},
	title = {Reinforcement Learning: An Introduction},
	isbn = {978-0-262-25705-3},
	url = {http://ebookcentral.proquest.com/lib/ulbdarmstadt/detail.action?docID=3338821},
	shorttitle = {Reinforcement Learning},
	abstract = {Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives when interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability. The book is divided into three parts. Part I defines the reinforcement learning problem in terms of Markov decision processes. Part {II} provides basic solution methods: dynamic programming, Monte Carlo methods, and temporal-difference learning. Part {III} presents a unified view of the solution methods and incorporates artificial neural networks, eligibility traces, and planning; the two final chapters present case studies and consider the future of reinforcement learning.},
	publisher = {{MIT} Press},
	author = {Sutton, R. and Barto, A.},
	date = {2015}
}

@inproceedings{strens_2000,
	location = {San Francisco, {CA}, {USA}},
	title = {A Bayesian Framework for Reinforcement Learning},
	isbn = {978-1-55860-707-1},
	url = {http://dl.acm.org/citation.cfm?id=645529.658114},
	series = {{ICML} '00},
	pages = {943--950},
	booktitle = {Proceedings of the Seventeenth International Conference on Machine Learning},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Strens, M.},
	date = {2000}
}

@article{auer_ucb,
	title = {Finite-time Analysis of the Multiarmed Bandit Problem},
	abstract = {Reinforcement learning policies face the exploration versus exploitation dilemma, i.e. the search for a balance between exploring the environment to ﬁnd proﬁtable actions while taking the empirically best action as often as possible. A popular measure of a policy’s success in addressing this dilemma is the regret, that is the loss due to the fact that the globally optimal policy is not followed all the times. One of the simplest examples of the exploration/exploitation dilemma is the multi-armed bandit problem. Lai and Robbins were the ﬁrst ones to show that the regret for this problem has to grow at least logarithmically in the number of plays. Since then, policies which asymptotically achieve this regret have been devised by Lai and Robbins and many others. In this work we show that the optimal logarithmic regret is also achievable uniformly over time, with simple and efﬁcient policies, and for all reward distributions with bounded support.},
	pages = {235-–256},
	volume = {47},
	journaltitle = {Machine Learning},
	shortjournal = {Machine Learning},
	author = {Auer, P. and Cesa-Bianchi, N.},
	date = {2002},
	langid = {english}
}

@inproceedings{kocsis_szepesvari_uct,
	title = {Bandit Based Monte-Carlo Planning},
	isbn = {978-3-540-46056-5},
	series = {Lecture Notes in Computer Science},
	abstract = {For large state-space Markovian Decision Problems Monte-Carlo planning is one of the few viable approaches to find near-optimal solutions. In this paper we introduce a new algorithm, {UCT}, that applies bandit ideas to guide Monte-Carlo planning. In finite-horizon or discounted {MDPs} the algorithm is shown to be consistent and finite sample bounds are derived on the estimation error due to sampling. Experimental results show that in several domains, {UCT} is significantly more efficient than its alternatives.},
	pages = {282--293},
	booktitle = {Machine Learning: {ECML} 2006},
	publisher = {Springer Berlin Heidelberg},
	author = {Kocsis, L. and Szepesvári, C.},
	editor = {Fürnkranz, J. and Scheffer, T. and Spiliopoulou, M.},
	date = {2006},
	langid = {english}
}

@inproceedings{abbasi,
	title = {Linear Programming for Large-Scale Markov Decision Problems},
	abstract = {We consider the problem of controlling a Markov decision  process ({MDP}) with a large state space, so as to minimize average cost.  Since it is intractable to compete with the optimal policy for lar...},
	eventtitle = {International Conference on Machine Learning},
	pages = {496--504},
	booktitle = {International Conference on Machine Learning},
	author = {Abbasi-Yadkori, Y. and Bartlett, P. and Malek, A.},
	date = {2014},
	langid = {english}
}

@thesis{farias,
	title = {The Linear Programming Approach to Approximate Dynamic Programming: Theory and Application},
	institution = {Stanford University},
	type = {phdthesis},
	author = {de Farias, D. P.},
	date = {2002}
}
